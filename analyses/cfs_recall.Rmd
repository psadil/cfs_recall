---
title: "CFS Recall (Master's Expt 2), variable transparency"
author: "pss"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
params:
  data_dir: data
  expt: CFSgonogo
  cutoff: 1
  percentiles: 4
  exclude: !r c(NaN)
  low_name: !r c(28, 30, 45, 54)
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, fig.path = paste0(file.path('output','figures',params$expt),.Platform$file.sep), cache = TRUE, message = FALSE, fig.width = 14, fig.asp = 1)

library(tidyverse)
library(magrittr)
library(stringdist)
library(Hmisc)
library(ggridges)
library(modelr)
library(broom)
library(lme4)
library(lsmeans)

source(file.path('utils.R'))
targets <- read_csv('objectNames_2afc.csv') 

out <- list()
for (i in 1:dim(targets[1])){
  out[i] = list(c(targets[i,]$name1, targets[i,]$name2, targets[i,]$name3, targets[i,]$name4, targets[i,]$name5, targets[i,]$name6, targets[i,]$name7, targets[i,]$name8, targets[i,]$name9, targets[i,]$name10, targets[i,]$name11, targets[i,]$name12, targets[i,]$name13, targets[i,]$name14, targets[i,]$name15))
}

cbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

Conditions = c('Not Studied','CFS, Image', 'Binocular, Image')

colmap <- c('Not Studied'=cbPalette[7]
            ,'Word'=cbPalette[2]
            ,'CFS'=cbPalette[3]
            ,'Binocular'=cbPalette[4])

old_theme <- theme_set(theme_classic(base_size = 12)) +
  theme_update(
    axis.ticks = element_blank()
    , axis.line = element_line(size=1)
    , plot.margin=margin(rep(0,4))
  )

```


```{r loadData, message=FALSE}

s_dirs <- list.files(path=params$data_dir)
files <- list.files(path=file.path(params$data_dir, s_dirs, params$expt), pattern = ".csv", full.names = TRUE)
files <- files[!grepl(glue::glue('subject_{params$exclude}_'), files)]
d <- lapply(files, read_csv, col_types=cols(pas_1=col_character(),pas_2=col_character(), pas = col_character())) %>% 
  bind_rows() %>%
  select(subject, pas_1, pas_2, item_test, tType_study, item, name_test, gonogo_answer, response_cue, response_noise, rt_noise) %>%
  group_by(subject) %>%
  nest() %>%
  mutate(data = map(data, . %>% mutate(test_in_study = match(item_test, item)))) %>%
  mutate(data = map(data, . %>% mutate(Condition = tType_study[test_in_study]))) %>%
  mutate(data = map(data, . %>% mutate(pas_1 = pas_1[test_in_study]))) %>%
  mutate(data = map(data, . %>% mutate(pas_2 = pas_2[test_in_study]))) %>%
  unnest() %>%
  mutate(Condition = factor(Condition, levels = c("Not Studied","CFS","Binocular"))) %>%
  select(-item, -tType_study, -test_in_study) %>%
  mutate(noise_correct = if_else((gonogo_answer=='go' & response_noise=='Return') |
                                   (gonogo_answer=='nogo' & response_noise!='Return'), 1, 0)) %>%
  mutate(subject=factor(subject)) %>%
  mutate(targets = out[item_test]) %>%
  mutate(firstTarget=sapply(X=targets, FUN=function(x) extract2(x,1))) %>%
  mutate(dl = Map(function(x,y) stringdist(x, y, method="dl"), targets, response_cue)) %>%
  mutate(minDist = sapply(X=dl, FUN=function(x) min(x, na.rm=TRUE))) %>%
  mutate(cue_correct = if_else(minDist < params$cutoff, 1, 0)) %>%
  mutate(pas_1 = as.numeric(substring(pas_1,1,1))) %>%
  mutate(pas_1 = if_else(is.na(pas_1),0,pas_1)) %>%
  mutate(pas_1 = factor(pas_1)) %>%
  mutate(pas_2 = as.numeric(substring(pas_2,1,1))) %>%
  mutate(pas_2 = if_else(is.na(pas_2),0,pas_2)) %>%
  mutate(pas_2 = factor(pas_2)) %>%
  mutate(id = 1:n()) %>%
  mutate(cue_correct_fct = factor(cue_correct))

  

# This method relies on saving the data and looking at each response to each cue individually. A response may have been marked as incorrect automatically, but could still be semantically related to the target. Those need to be corrected manually, and then the targets need to be updated.

# d %>%
#   mutate(id = 1:n()) %>%
#   select(firstTarget, response_cue, minDist, cue_correct, id) %>%
#   filter(cue_correct == 0) %>%
#   write_csv(x=., path = file.path("tmp.csv"))

```


`r n_distinct(d$subject) - length(params$low_name)` participants worth of data usable data (`r n_distinct(d$subject)` finished the experiment in total, but `r length(params$low_name)` have about 0% correct on naming trials. Note that the participant IDs don't quite match up to the total nunber of participants. The mismatch is because the RAs accidentally skipped a couple of numbers), recruited during the summer and through SONA. We're had been aiming for 60.

`r n_distinct(d$item_test)` items per participant, encountered in lists of 12 items. With three conditions (binocular, cfs, not-studied), there were four items in each condition in each list. Of those four items, three were encountered in a “go” test trial, and one was in a “no-go” test trial. So, 1/4 of test-trials were no-go. This leaves up to 30 items in go trials per condition, per participant (but the actual numer will be different, depending on how many objects were correctly named). 

## Figures

### Naming Accuracy

The following plot shows the proportion of items that participants named correctly. The first plot shows each participant invididually to see which ones didn't try during the naming condition. These ones were excluded from the group-level plot and all further plots.

Note that in this group-level plot (and all other following), each participants' average performance is calculated separetely and then averaged together. Error bars reflect the standard error of the mean, averaging across participant averages (note that I had previously been showing 95% CI).


```{r naming}

d %>%
  ggplot(aes(x=Condition, y=cue_correct, color = Condition)) +
  stat_summary(fun.data="mean_se") +
  scale_color_manual(values=colmap, name= "Condition") +  
  facet_wrap(~subject, nrow = 3) +
  geom_hline(yintercept = 0) +
  geom_hline(yintercept = .3, linetype = "dashed") +
  scale_y_continuous(limits = c(0,1), breaks = c(0,.5,1), name = "Naming Accuracy") +
  scale_x_discrete(name = NULL, label = element_blank()) 

d %<>% filter(!(subject %in% params$low_name)) %>% mutate(subject = forcats::fct_drop(subject))

d %>%
  group_by(subject, Condition) %>%
  summarise(cue_correct = mean(cue_correct)) %>%
  ggplot(aes(x=Condition, y=cue_correct, color = Condition)) +
  stat_summary(fun.data="mean_se") +
  scale_y_continuous(limits = c(0,1), name = "Naming Accuracy") +
  scale_x_discrete(name = NULL, label = element_blank()) +
  scale_color_manual(values=colmap, name= "Condition") +
  geom_hline(yintercept = .3, linetype = "dashed")



```

### PAS

Here are the PAS ratings provided to the first and second encounter of items in the study phase. Only items studied in the CFS condition are shown, because these are the only trials in which participants were asked to provide a PAS rating.

Most participants gave ratings of 2 or 3 on most CFS trials. A few were better at stopping the trial before being able to identify the object (provided mostly PAS 2 ratings), but I wouldn't rule out that those participants were simply pressing 2 because they thought that was the 'correct' response.

```{r pas}

d %>%
  gather(key = rating, value = PAS, pas_1:pas_2) %>%
  filter(Condition  == "CFS") %>%
  ggplot(aes(x = PAS, fill = PAS)) +
  facet_grid( Condition ~ rating) +
  geom_bar()
  
d %>%
  gather(key = rating, value = PAS, pas_1:pas_2) %>%
  filter(Condition=="CFS") %>%
  ggplot(aes(x = PAS, fill = PAS)) +
  geom_bar() +
  facet_wrap(~subject, nrow = 6)



``` 

### Go/No-go accuracy

The following plot shows proportion correct on the no/no-go trials (whether participants either said an item was appearing, or correctly waited). Raw accuracy is high. I didn't calculate anything like d' given that performance was basically at ceiling for all participants, and so d' would be infinity.

```{r noise_correct}

d %>%
  group_by(subject, Condition) %>%
  summarise(noise_correct = mean(noise_correct)) %>%
  ggplot(aes(x=Condition, y=noise_correct, color = Condition)) +
  stat_summary(fun.data="mean_se") +
  scale_y_continuous(limits = c(0,1), name = "Go/No-Go Accuracy") +
  scale_x_discrete(name = NULL, label = element_blank()) +
  scale_color_manual(values=colmap, name= "Condition") 
# +
#   ggsave('noise_accuracy_group.png')

d %>%
  ggplot(aes(x=Condition, y=noise_correct, color = Condition)) +
  stat_summary(fun.data="mean_se") +
  scale_x_discrete(name = NULL, label = element_blank()) +
  scale_color_manual(values=colmap, name= "Condition") +
  facet_wrap(~subject, nrow = 3) +
  geom_hline(yintercept = 0) +
  geom_hline(yintercept = 1, linetype = "dashed") +
  scale_y_continuous(limits = c(0,1), breaks = c(0,.5,1), name = "Naming Accuracy") 
# +
#   ggsave('noise_accuracy_sub.png', height = params$fig_size[2], width = params$fig_size[1]) 

```

### Percentiles

Percentiles are first calculated within participants, then averaged across participants.

Note that because these lines are conditioned on accurately naming the cue item, there are different numbers of trial in each condition. 


```{r percentiles}

p <- seq(from = 1 / params$percentiles, to = 1, length.out = params$percentiles)

d %>% filter(gonogo_answer == "go" 
                         & noise_correct==1) %>%
  group_by(Condition, cue_correct_fct, subject) %>%
  summarise(Percentile = list(p), 
            RT = list(quantile(rt_noise, probs = p, na.rm = TRUE))) %>%
  unnest() %>%
  ggplot(aes(x=Percentile, y=RT, color=Condition, linetype = cue_correct_fct)) +
  stat_summary(fun.data = 'mean_se') +
  stat_summary(fun.y = 'mean', geom="line") +
  scale_x_continuous(limits = c(0,1), breaks = p, name = "Percentile") +
  scale_y_continuous(limits = c(0,4), name = "RT (seconds) on (Correct) go trials") +
  scale_color_manual(values=colmap, name= "Condition") 



```

These cdf plots are shown for each participant

```{r percentiles_sub}

d %>% filter(gonogo_answer == "go" 
                         & noise_correct==1) %>%
  group_by(Condition, cue_correct_fct, subject) %>%
  summarise(Percentile = list(p), 
            RT = list(quantile(rt_noise, probs = p, na.rm = TRUE))) %>%
  unnest() %>%
  ggplot(aes(x=Percentile, y=RT, color=Condition, linetype=cue_correct_fct)) +
  geom_line() +
  scale_x_continuous(limits = c(0,1), breaks = p[seq.int(from=1,to=length(p), length.out = 3)], name = "CDF") +
  scale_y_continuous(limits = c(0,4), name = "RT (seconds) on (Correct) go trials") +
  scale_color_manual(values=colmap, name= "Condition")+
  facet_wrap(~subject, nrow=6) 


```

These next plots contain similar information as the ones above, but now conditions are shown as different from Not-Studied.

Note that in these participant-level plots, it's a little easier to see that this grouping of naming accuracy x condition -- combined with excluding trials where participants were incorrect in go-nogo trials -- leaves some participants without any data in some of the conditions. For example, participant 5 didn't name any of the objects in the Not Studied condition, so participant 5's graph doesn't have any 'cue_correct_fct' difference lines (there were no Not Studied items to use as a baseline).


```{r percentiles-diff}

d %>% filter(gonogo_answer == "go" 
                         & noise_correct==1) %>%
  group_by(Condition, cue_correct_fct, subject) %>%
  summarise(Percentile = list(p), 
            RT = list(quantile(rt_noise, probs = p, na.rm = TRUE))) %>%
  unnest() %>%
  ungroup() %>%
  spread(key = Condition, value = RT ) %>%
  mutate(CFS = `Not Studied` - CFS,
         Binocular = `Not Studied` - Binocular) %>%
  select(-`Not Studied`) %>%
  gather(key = Condition, value = RT, CFS:Binocular) %>%
  ggplot(aes(x=Percentile, y=RT, color=Condition, linetype = cue_correct_fct)) +
  stat_summary(fun.data = 'mean_se') +
  stat_summary(fun.y = 'mean', geom="line") +
  scale_x_continuous(limits = c(0,1), breaks = p, name = "Percentile") +
  scale_y_continuous(limits = c(-.5,.5), name = "RT (seconds) Speed-up (Not Studied - Condition) on (Correct) go trials") +
  scale_color_manual(values=colmap, name= "Condition") +
  geom_hline(yintercept = 0, linetype = "dashed")


d %>% filter(gonogo_answer == "go" 
                         & noise_correct==1) %>%
  group_by(Condition, cue_correct_fct, subject) %>%
  summarise(Percentile = list(p), 
            RT = list(quantile(rt_noise, probs = p, na.rm = TRUE))) %>%
  unnest() %>%
  ungroup() %>%
  spread(key = Condition, value = RT ) %>%
  mutate(CFS = `Not Studied` - CFS,
         Binocular = `Not Studied` - Binocular) %>%
  select(-`Not Studied`) %>%
  gather(key = Condition, value = RT, CFS:Binocular) %>%
  ggplot(aes(x=Percentile, y=RT, color=Condition, linetype = cue_correct_fct)) +
  scale_x_continuous(limits = c(0,1), breaks = p, name = "Percentile") +
  scale_y_continuous(name = "RT (seconds) Speed-up (Not Studied - Condition) on (Correct) go trials") +
  scale_color_manual(values=colmap, name= "Condition") +
  facet_wrap(~subject, nrow=6) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_line()


```

The above plots have just 4 percentiles, because that was how many were used in a paper Dave sent me. Previously, I had been grouping the data into 5 percentiles. Below are the different plots with 5 percentiles.

```{r percentiles-5}

p <- seq(from = 1 / params$percentiles, to = 1, length.out = 5)

d %>% filter(gonogo_answer == "go" 
                         & noise_correct==1) %>%
  group_by(Condition, cue_correct_fct, subject) %>%
  summarise(Percentile = list(p), 
            RT = list(quantile(rt_noise, probs = p, na.rm = TRUE))) %>%
  unnest() %>%
  ungroup() %>%
  spread(key = Condition, value = RT ) %>%
  mutate(CFS = `Not Studied` - CFS,
         Binocular = `Not Studied` - Binocular) %>%
  select(-`Not Studied`) %>%
  gather(key = Condition, value = RT, CFS:Binocular) %>%
  ggplot(aes(x=Percentile, y=RT, color=Condition, linetype = cue_correct_fct)) +
  stat_summary(fun.data = 'mean_se') +
  stat_summary(fun.y = 'mean', geom="line") +
  scale_x_continuous(limits = c(0,1), breaks = p, name = "Percentile") +
  scale_y_continuous(limits = c(-.5,.5), name = "RT (seconds) Speed-up (Not Studied - Condition) on (Correct) go trials") +
  scale_color_manual(values=colmap, name= "Condition")  +
  geom_hline(yintercept = 0, linetype = "dashed")


d %>% filter(gonogo_answer == "go" 
                         & noise_correct==1) %>%
  group_by(Condition, cue_correct_fct, subject) %>%
  summarise(Percentile = list(p), 
            RT = list(quantile(rt_noise, probs = p, na.rm = TRUE))) %>%
  unnest() %>%
  ungroup() %>%
  spread(key = Condition, value = RT ) %>%
  mutate(CFS = `Not Studied` - CFS,
         Binocular = `Not Studied` - Binocular) %>%
  select(-`Not Studied`) %>%
  gather(key = Condition, value = RT, CFS:Binocular) %>%
  ggplot(aes(x=Percentile, y=RT, color=Condition, linetype = cue_correct_fct)) +
  scale_x_continuous(limits = c(0,1), breaks = p, name = "Percentile") +
  scale_y_continuous(name = "RT (seconds) Speed-up (Not Studied - Condition) on (Correct) go trials") +
  scale_color_manual(values=colmap, name= "Condition") +
  facet_wrap(~subject, nrow=6) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_line()

```


```{r lmer}
p <- seq(from=1 / params$percentiles, to=1, length.out = params$percentiles)

rt_quant0 <- d %>% filter(gonogo_answer == "go"
                         & noise_correct==1) %>%
  group_by(Condition, cue_correct_fct, subject) %>%
  summarise(Percentile = list(p), RT = list(quantile(rt_noise, probs = p, na.rm = TRUE))) %>%
  unnest() %>%
  ungroup() %>%
  mutate(Percentile = factor(Percentile),
         Condition = factor(Condition)) %>%
  spread(key = Condition, value = RT ) %>%
  mutate(CFS = `Not Studied` - CFS,
         Binocular = `Not Studied` - Binocular) %>%
  select(-`Not Studied`) %>%
  gather(key = Condition, value = RT, CFS:Binocular)


rt_mod0 <- afex::mixed(RT ~ Condition*Percentile*cue_correct_fct + (1 | subject), data = rt_quant0, type = 3)

m0 <- lsmeans::lsmeans(rt_mod0, ~Condition|cue_correct_fct, weights = "proportional")
m2 <- lsmeans::lsmeans(rt_mod0, ~Condition|cue_correct_fct:Percentile, at = list(Percentile = "0.25"), weights = "proportional")
```

For the actual stats, I ran a linear-mixed effects model to test for 1) an interaction between study condition x percentile when object isn't named, 2) a post-hoc contrast to test for an effect of CFS when object isn't named, and 3) a test for an effect of CFS when object isn't named, at the first quantile. 

These ended up being as we wanted them. There was no interaction between condition x percentile, but there was an effect of study under CFS (collapsing across percentiles), and that includes an effect os study at the first percentile.

```{r overall}
print(rt_mod0)
```


```{r table1}
summary(m0, infer = c(TRUE, TRUE), adjust = "scheffe")
```

```{r table2 }
summary(m2, infer = c(TRUE, TRUE), adjust = "scheffe")
```


### Extra details about mixed-model

Here are a couple of details about how I specified the mixed-effects model. I fit the model with the R package lme4 (I think we had a lab meeting dedicated to fitting mixed-effects model with this package a while ago). I assumed fixed effects for the Condition, Percentile, and naming accuracy. Percentiles were treated as a categorical (not continuous) variable. The model I fit included interactions for all of these terms. Additionally, I included a random intercept for participants. However, I did not include any random slopes (i.e., no interaction between participant and the other predictors). Finally, following Kevin's advice, I fit the model to differences of quantiles, rather than the quantiles directly. 



## Fitting exponentially modified Gaussian distribution

This next section contains some preliminary results from fitting an exponentially modified gaussian distribution to the RTs. As above, I only looked at 'go' trials in which participants correctly responded go. The three parameters of the exgaussian were fit using a Bayesian Heirarchical framework. As with the mixed-model above, I always coded participants as random effects (i.e., each participant had their own unique intercept, and these intercept were drawn from a single hyperprior distribution which was normally distributed and whose variance was itself estimated). I then considered just a few different combinations of model parameters. The model with the most parameters that I looked at allowed the location and variance of the guassian part of the exgaussian -- as well as the rate parameter of the exponential -- to vary by Condition, whether the aperature was named correctly, and an interaction between the two. I fit two reduced models, one in which the Condition did not affect the mean of the exgaussian and another in which the Condition did not affect either the rate or variance parameters.

```{r, brms_init}
library(brms)
options(mc.cores = 2)

exg_brms_d <- d %>% filter(gonogo_answer == "go"
                        & noise_correct==1)

# exg_brms_prior <-  brms::brm(bf(rt_noise ~ cue_correct_fct*Condition + (1 | subject), sigma ~ cue_correct_fct*Condition, beta ~ cue_correct_fct*Condition
#                           , family = exgaussian() ),
#                        data = exg_brms_d,
#                        prior = c(prior(student_t(3,0,1), class = "b"),
#                                  prior(student_t(3,0,1), class = "Intercept"),
#                                  prior(gamma(2,1), class = "sd", group = "subject"),
#                                  prior(student_t(3,0,1), class = "b", dpar = "sigma"),
#                                  prior(student_t(3,0,1), class = "Intercept", dpar = "sigma"),
#                                  prior(student_t(3,0,1), class = "b", dpar = "beta"),
#                                  prior(student_t(3,0,1), class = "Intercept", dpar = "beta")),
#                        chains = 2,
#                        save_model = "prior.stan",
#                        sample_prior = "only")
# 
# tmp <- plot(marginal_effects(exg_brms_prior), points = FALSE, ask = FALSE)
# 
# tmp$`cue_correct_fct:Condition` +
#   coord_cartesian(ylim=c(0, 4))

```


```{r, brms}

exg_brms_full <-  brms::brm(bf(rt_noise ~ cue_correct_fct*Condition + (1 | subject), sigma ~ cue_correct_fct*Condition, beta ~ cue_correct_fct*Condition
                               , family = exgaussian() ),
                            data = exg_brms_d,
                            prior = c(prior(student_t(3,0,1), class = "b"),
                                      prior(student_t(3,0,1), class = "Intercept"),
                                      prior(gamma(2,1), class = "sd", group = "subject"),
                                      prior(student_t(3,0,1), class = "b", dpar = "sigma"),
                                      prior(student_t(3,0,1), class = "Intercept", dpar = "sigma"),
                                      prior(student_t(3,0,1), class = "b", dpar = "beta"),
                                      prior(student_t(3,0,1), class = "Intercept", dpar = "beta")),
                            chains = 2,
                            save_model = "test1.stan")

exg_brms_full_noCond <-  brms::brm(bf(rt_noise ~ cue_correct_fct + (1 | subject), sigma ~ cue_correct_fct*Condition, beta ~ cue_correct_fct*Condition
                               , family = exgaussian() ),
                            prior = c(prior(student_t(3,0,1), class = "b"),
                                      prior(student_t(3,0,1), class = "Intercept"),
                                      prior(gamma(2,1), class = "sd", group = "subject"),
                                      prior(student_t(3,0,1), class = "b", dpar = "sigma"),
                                      prior(student_t(3,0,1), class = "Intercept", dpar = "sigma"),
                                      prior(student_t(3,0,1), class = "b", dpar = "beta"),
                                      prior(student_t(3,0,1), class = "Intercept", dpar = "beta")),
                            data = exg_brms_d,
                            chains = 2,
                            save_model = "test3.stan")


exg_brms_reduced <-  brms::brm(bf(rt_noise ~ cue_correct_fct*Condition + (1 | subject), sigma ~ cue_correct_fct, beta ~ cue_correct_fct
                                  , family = exgaussian() ),
                               data = exg_brms_d,
                               prior = c(prior(student_t(3,0,1), class = "b"),
                                         prior(student_t(3,0,1), class = "Intercept"),
                                         prior(gamma(2,1), class = "sd", group = "subject"),
                                         prior(student_t(3,0,1), class = "b", dpar = "sigma"),
                                         prior(student_t(3,0,1), class = "Intercept", dpar = "sigma"),
                                         prior(student_t(3,0,1), class = "b", dpar = "beta"),
                                         prior(student_t(3,0,1), class = "Intercept", dpar = "beta")),
                               chains = 2,
                               save_model = "test_red.stan")


```

```{r, waic}

waic_full <- brms::WAIC(exg_brms_full, exg_brms_reduced, exg_brms_full_noCond)

```

### WAIC for model comparison

As a first pass to see the affect of condition, I tried a model comparison approach to see whether study condition affected either the location parameter or one of the other two parameters. This revealed that the addition of parameters to allow a study condition-dependent shift in RT meaningfully increased the fit of the model. However, allowing the rate parameter in the exponential component or the variance of the normal component to vary did not meaningfully increase the model fit. 

There are many combinations of parameters that I haven't looked at (e.g., allowing just one of either the rate or the variance to shift by condition). How many of these do you think it's important to look at? For example, I could consider looking closer at the rate of the exponential.


### Posterior Predictive Check Summaries

Here are three plots to assess the fit of the model (given the WAIC of the models, I'm only using a model where study condition affects the location of the exgaussian, but doesn't affect the variance or rate parameters). The first plot is a histogram of the residual error of the data as compared to 12 iterations of sampling from the posterior distribution of the model. That is, the posterior distribution of parameter fits were used to generate 12 different datasets, each with the same number of observations as the original data. Each of the 12 histograms show the results of subtracting one of these simulated datasets from the original data. If the model is capturing all major trends, the histograms should be approximately normally distributed, centered at 0. If the model wasn't capturing major trends in the data, the histograms would appear systematically non-normal or shifted  (e.g., if they were centered on a value larger than 1, it would suggest that the model consistently predicted too low of RTs).

The second and third plots are much like the first, but the plots are broken into the relevant fixed effects, the interpretation is the same as the first plot, but this is just a check to make sure that all groups were being fit equally well. 

Note that the third plot breaks the data into whether or not the aperture was named correctly (named == 1). When collapsed across conditions, most objects weren't named, so those bars on the right are much smaller than those on the left.

```{r pp_check}

brms::pp_check(exg_brms_reduced, type = "error_hist", nsamples = 12, binwidth = 10/30)

brms::pp_check(exg_brms_reduced, type = "error_hist_grouped", nsamples = 3, group = "Condition", binwidth = 10/30)

brms::pp_check(exg_brms_reduced, type = "error_hist_grouped", nsamples = 3, group = "cue_correct_fct", binwidth = 10/30)
```

### Posterior Distribution

Here is a quick summary of the posterior distribution of the effect of study on RT (using the model where Condition only affected the location of the exgaussian). This shows the median of the posterior, with error bars extending to from the 2.5 to 97.5 quantiles of the posterior. These distributions being below 0 indicates a significant speed-up.

```{r marginals}

### Plots of posterior of fixed-effects

# Here are 3 plots to look at the posterior distribution of the two fixed effects' location parameters, as well as their interaction's effect on location. On each plot, the dot indicates a median, and the error bars extend to the 2.5 and 97.5 percent highest density interval. Only the winning model is shown (Condition effected location of the exgaussian, but not variance or rate parameters). The relevant comparison is 
# 
# 
# plot(marginal_effects(exg_brms_reduced), points = FALSE, ask = FALSE)

post <- as_tibble(as.data.frame(exg_brms_reduced)) %>%
  select(b_ConditionCFS, b_ConditionBinocular) %>%
  gather(key = Condition, value = Posterior, b_ConditionCFS:b_ConditionBinocular) %>%
  mutate(Condition = plyr::mapvalues(Condition, from = c("b_ConditionCFS", "b_ConditionBinocular"), to = c("CFS", "Binocular"))) %>%
  mutate(Condition = factor(Condition, levels = c("Not Studied","CFS","Binocular")))

post %>%
  ggplot(aes(x = Condition, y = Posterior)) +
  stat_summary(fun.data = "median_hilow") +
  scale_y_continuous(limits = c(-.5,.5), name = "Posterior of effect on RT") +
  geom_hline(yintercept = 0, linetype = "dashed")
```
